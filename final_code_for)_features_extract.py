# -*- coding: utf-8 -*-
"""final_code

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DbIKqfplvuw5Hjn8Joq5VtDXMrU9SLZm
"""

import shutil
import subprocess
import glob
from tqdm import tqdm
import numpy as np
import cv2
import os
import re
import librosa.display
from keras.layers import Flatten, Input
from keras.applications.resnet import ResNet152
from keras.models import Model 
from numpy.linalg import norm
import moviepy.editor

class VideoCaptioningPreProcessing:

    def __init__(self,video_dest=r"/content/drive/MyDrive/TestVideo",feat_dir=r"/content/drive/MyDrive/features",
                 temp_dest=r"/content/drive/MyDrive/frames",img_dim=224,channels=3,
                 batch_size=128,frames_step=80):

        self.img_dim = img_dim
        self.channels = channels
        self.video_dest = video_dest
        self.feat_dir = feat_dir
        self.temp_dest = temp_dest
        self.batch_cnn = batch_size
        self.frames_step = frames_step

    def video_to_frames(self,video):

          with open(os.devnull, "w") as ffmpeg_log:
            if os.path.exists(self.temp_dest):
                print(" cleanup: " + self.temp_dest + "/")
                shutil.rmtree(self.temp_dest)
            os.makedirs(self.temp_dest)
            video_to_frames_cmd = ["ffmpeg",'-y','-i', video,  '-vf', "scale=400:300", '-qscale:v', "2",'{0}/%06d.jpg'.format(self.temp_dest)]
            subprocess.call(video_to_frames_cmd,
                            stdout=ffmpeg_log, stderr=ffmpeg_log)

# Load the pre-trained Model and extract the dense features as output 
    def model_cnn_load(self):
         model = ResNet152(weights = "imagenet", include_top=True,input_shape =(self.img_dim,self.img_dim,self.channels))
         out = model.layers[-2].output
         model_final = Model(inputs=model.input,outputs=out)
         return model_final

# Load the video images 

    def load_image(self,path):
        img = cv2.imread(path)
        img = cv2.resize(img,(self.img_dim,self.img_dim))
        return img


    def extract_audio(self,i,filepath_for_dataset) :

        os.chdir(filepath_for_dataset)
        k=str(i)+".mp4"
        my_clip = moviepy.editor.VideoFileClip(k)
        if not os.path.isdir(r'/content/drive/MyDrive/audio'):
            os.mkdir(r'/content/drive/MyDrive/audio')
        try :
              os.chdir(r'/content/drive/MyDrive/audio')
              l=str(i)+".mp3"
              my_clip.audio.write_audiofile(l)
              return True
        except :
              print("no audio")
              return False

    def file_to_audiofeature(self,sfname):
        audio_y, sr = librosa.load(sfname, res_type='kaiser_fast')
        afeatures = librosa.feature.mfcc(y=audio_y, sr=sr, n_mfcc=40)
        afeatures = np.transpose(afeatures)
        #print(afeatures.shape)
        ll = len(afeatures)
        parts = ll / 80
        #print(parts)
        division = []
        for i in range(80 - 1):
            division.append(int((i + 1) * parts))
        for i in range(ll % 80):  # left over
            division[i] += 1
        division = np.array(division)
    
        afeatures = np.split(np.array(afeatures), division)
        afeature_out = []
        for af in afeatures:
            afeature_out.append(np.mean(np.array(af), axis=0))
        afeature_out = np.asarray(afeature_out)
        if np.shape(afeature_out) != (80, 40):
             print("File [%s] with audio problem (%s)" % (sfname, str(np.shape(afeature_out))))
             # Ignore videos
             os.system("touch %s_ignore" % sfname)
        return afeature_out
    

# Extract the features from the pre-trained CNN    
    def extract_feats_pretrained_cnn(self):

        model = self.model_cnn_load()
        print('Model loaded')

        if not os.path.isdir(self.feat_dir):
            os.mkdir(self.feat_dir)
        #print("save video feats to %s" % (self.dir_feat))
        video_list = glob.glob(os.path.join(self.video_dest, '*.mp4'))
        video_list=sorted(video_list,key=lambda x:float(re.findall("(\d+)",x)[0]))
       
    
    

        for video in tqdm(video_list):
            video_id = video.split("/")[-1].split(".")[0]
            num_id=""
            for num in video_id :
                if num.isnumeric() :
                    num_id+=num
                    
            if  int(num_id) >= 7010 :
                        
                        print(f'Processing video {video}')
                        if int(num_id)== 7050:
                               break
            
                        #self.dest = 'cnn_feat' + '_' + video_id
                        self.video_to_frames(video)
                        image_list = sorted(glob.glob(os.path.join(self.temp_dest, '*.jpg')))
                        samples = np.round(np.linspace(0, len(image_list) - 1,self.frames_step))
                        image_list = [image_list[int(sample)] for sample in samples]
                        images = np.zeros((len(image_list),self.img_dim,self.img_dim,self.channels))
                        for i in range(len(image_list)):
                                      img = self.load_image(image_list[i])
                                      images[i] = img
                        images = np.array(images)
                        fc_feats = model.predict(images,batch_size=self.batch_cnn)
                        img_feats = np.array(fc_feats)
                        filename=video_id + '.npy'
                        outfile = os.path.join(self.feat_dir,filename)
                        
                        if self.extract_audio(video_id,self.video_dest) :
                                     X= self.file_to_audiofeature(os.path.join(r"/content/drive/MyDrive/audio/"+video_id+'.mp3'))
                                     img_feats = np.hstack((img_feats, X))
                                     np.save(outfile, img_feats)
                                     print("the array shape is {}".format(img_feats.shape))
                                    
                        else :        
                                     print("no audio features")
                                     kk=np.zeros((80,40))
                                     img_feats = np.hstack((img_feats, kk))
                                     np.save(outfile, img_feats)
                                     print("the array shape is {} with no audio".format(img_feats.shape))
                                    

                        # cleanup
                        shutil.rmtree(self.temp_dest)
                        shutil.rmtree("/content/drive/MyDrive/audio")

if __name__ == '__main__':
        v=VideoCaptioningPreProcessing()
        v.extract_feats_pretrained_cnn()

